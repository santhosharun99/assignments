{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0NbemMgRusj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import EMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Download and load the EMNIST \"Balanced\" dataset\n",
        "emnist_train = EMNIST(root='emnist_data', split='balanced', train=True, transform=ToTensor(), download=True)\n",
        "emnist_test = EMNIST(root='emnist_data', split='balanced', train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "# Split the train data into train and validation sets\n",
        "train_len = int(len(emnist_train) * 0.8)\n",
        "valid_len = len(emnist_train) - train_len\n",
        "train_dataset, valid_dataset = random_split(emnist_train, [train_len, valid_len])\n",
        "\n",
        "# Create DataLoaders for train, validation, and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(emnist_test, batch_size=64, shuffle=False)\n",
        "\n",
        "def show_example(img, label):\n",
        "    plt.imshow(img.squeeze().numpy(), cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.show()\n",
        "\n",
        "# Display a few examples from the train dataset\n",
        "for i in range(5):\n",
        "    img, label = emnist_train[i]\n",
        "    show_example(img, label)\n",
        "\n",
        "print(f\"Number of training samples: {len(emnist_train)}\")\n",
        "print(f\"Number of testing samples: {len(emnist_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP (Multilayer Perceptron) class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, activation='relu', dropout_rate=0.0, batch_normalization=True, l1_reg=0.0):\n",
        "      # Initialize the MLP with given hyperparameters\n",
        "        super(MLP, self).__init__()\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 47)\n",
        "        # Define dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # Set batch normalization flag\n",
        "        self.batch_normalization = batch_normalization\n",
        "        self.l1_reg = l1_reg\n",
        "        # Initialize batch normalization layers if enabled\n",
        "        if self.batch_normalization:\n",
        "            self.bn1 = nn.BatchNorm1d(128)\n",
        "            self.bn2 = nn.BatchNorm1d(64)\n",
        "            self.bn3 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Set the activation function based on the given parameter\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = nn.LeakyReLU()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "\n",
        "    # Define forward pass for the MLP\n",
        "    def forward(self, x):\n",
        "      # Flatten input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # Apply first fully connected layer, batch normalization (if enabled), activation, and dropout\n",
        "        x = self.fc1(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply fourth fully connected layer\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Define the CNN (Convolutional Neural Network) class\n",
        "class CNN(nn.Module):\n",
        "  # Initialize the CNN with given hyperparameters\n",
        "    def __init__(self, activation='relu', dropout_rate=0.0, batch_normalization=True, l1_reg=0.0):\n",
        "        super(CNN, self).__init__()\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 47)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.batch_normalization = batch_normalization\n",
        "        self.l1_reg = l1_reg\n",
        "        # Add batch normalization layers if specified\n",
        "        if self.batch_normalization:\n",
        "            self.bn1 = nn.BatchNorm2d(32)\n",
        "            self.bn2 = nn.BatchNorm2d(64)\n",
        "            self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Choose the activation function based on the provided activation type\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = nn.LeakyReLU()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "\n",
        "    # Define the forward pass of the CNN\n",
        "    def forward(self, x):\n",
        "      # First convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        # Apply batch normalization if specified\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn1(x)\n",
        "        x = self.activation(x)  # Apply the activation function\n",
        "        # Apply max pooling\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        # First fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        if self.batch_normalization:\n",
        "            x = self.bn3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        # Second fully connected layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "#create functions to create MLP and CNN models with given hyperparameters passed as the 'params' argument. \n",
        "def create_mlp(params):\n",
        "     return MLP(activation=params['activation'], dropout_rate=params['dropout_rate'], batch_normalization=params['batch_normalization'], l1_reg=params['l1_reg'])\n",
        "\n",
        "def create_cnn(params):\n",
        "     return CNN(activation=params['activation'], dropout_rate=params['dropout_rate'], batch_normalization=params['batch_normalization'], l1_reg=params['l1_reg'])\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, criterion, scheduler, epochs, params):\n",
        "    # Set the model to train mode\n",
        "    model.train()\n",
        "    # Initialize lists to store training and validation losses and accuracies\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_losses = []\n",
        "    valid_accuracies = []\n",
        "\n",
        "    # Loop through all epochs\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "          # Loop through all batches in the training dataset\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad() # Zero the gradients of the optimizer\n",
        "            outputs = model(inputs)  # Forward pass through the model\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Add L1 regularization if needed\n",
        "            if params['l1_reg'] > 0:\n",
        "                l1_regularization = torch.tensor(0.).to(device)\n",
        "                for param in model.parameters():\n",
        "                    l1_regularization += torch.norm(param, 1)\n",
        "                loss += params['l1_reg'] * l1_regularization\n",
        "            loss.backward() # Backpropagate the gradients\n",
        "            optimizer.step() # Update the weights\n",
        "\n",
        "            # Update the running loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step() # Update the learning rate scheduler\n",
        "\n",
        "        # Calculate and store average training loss and accuracy for the current epoch\n",
        "        train_losses.append(running_loss / (i + 1))\n",
        "        train_accuracies.append(100 * correct / total)\n",
        "\n",
        "        # Evaluate the model on the validation dataset and store the loss and accuracy\n",
        "        valid_loss, valid_accuracy = evaluate_model(model, valid_loader)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        # Evaluate the model on the validation dataset and store the loss and accuracy\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]}, Validation Accuracy: {valid_accuracies[-1]}%\")\n",
        "\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
        "\n",
        "def evaluate_model(model, valid_loader):\n",
        "  # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs) # Forward pass through the model\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update the running accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate the validation loss and accuracy\n",
        "    valid_accuracy = 100 * correct / total\n",
        "    valid_loss = running_loss / len(valid_loader)\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "def create_optimizer(model, params):\n",
        "  # Create the specified optimizer with provided hyperparameters\n",
        "    if params['optimizer'] == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['l2_reg'])\n",
        "    elif params['optimizer'] == 'ADAM':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['l2_reg'])\n",
        "    elif params['optimizer'] == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=params['lr'], weight_decay=params['l2_reg'])\n",
        "    return optimizer\n",
        "\n",
        "def create_scheduler(optimizer, params):\n",
        "  # Create the specified optimizer with provided hyperparameters\n",
        "    if params['scheduler'] == 'StepLR':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    elif params['scheduler'] == 'ExponentialLR':\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "    return scheduler\n",
        "    \n",
        "criterion = nn.CrossEntropyLoss() # Define the loss criterion\n",
        "\n",
        "def cross_validation(create_model_func, train_dataset, k_folds, params, criterion):\n",
        "  # Initialize k-fold cross-validation\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "    results = []\n",
        "\n",
        "    # Iterate over each fold\n",
        "    for train_indices, valid_indices in kfold.split(train_dataset):\n",
        "      # Create training and validation subsets for the current fold\n",
        "        train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
        "        valid_subset = torch.utils.data.Subset(train_dataset, valid_indices)\n",
        "\n",
        "        # Create data loaders for the training and validation subsets\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Create the model and move it to the appropriate device\n",
        "        model = create_model_func(params)\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = create_optimizer(model, params) # Create the optimizer and learning rate scheduler\n",
        "\n",
        "        scheduler = create_scheduler(optimizer, params)\n",
        "\n",
        "        # Create the optimizer and learning rate scheduler\n",
        "        train_model(model, train_loader, valid_loader, optimizer, criterion, scheduler, params['epochs'], params)\n",
        "\n",
        "        # Evaluate the model on the validation subset and store the accuracy\n",
        "        _, accuracy = evaluate_model(model, valid_loader)\n",
        "        results.append(accuracy)\n",
        "\n",
        "    return sum(results) / len(results) # Return the average accuracy across all folds\n",
        "\n",
        "# Initial baseline configuration\n",
        "params = {\n",
        "    'optimizer': 'ADAM',\n",
        "    'lr': 0.01,\n",
        "    'l2_reg': 0.0001,\n",
        "    'l1_reg': 0,\n",
        "    'scheduler': 'StepLR',\n",
        "    'activation': 'relu',\n",
        "    'dropout_rate': 0.5,\n",
        "    'batch_normalization': True,\n",
        "    'epochs': 10\n",
        "}\n",
        "\n",
        "#Search for best set of hyperparameters\n",
        "def find_best_hyperparameters(model_fn, train_dataset, criterion, k_folds=2):\n",
        "\n",
        "    # Initialize best hyperparameters and accuracy\n",
        "    best_accuracy = 0\n",
        "    best_params = params.copy()\n",
        "\n",
        "    # Explore learning rate schedulers\n",
        "    schedulers = ['StepLR', 'ExponentialLR']\n",
        "    best_scheduler_accuracy = 0\n",
        "    best_scheduler = None\n",
        "\n",
        "    for scheduler in schedulers:\n",
        "        params['scheduler'] = scheduler\n",
        "        accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "        if accuracy > best_scheduler_accuracy:\n",
        "            best_scheduler_accuracy = accuracy\n",
        "            best_scheduler = scheduler\n",
        "\n",
        "    params['scheduler'] = best_scheduler\n",
        "\n",
        "    # Explore activation functions\n",
        "    activations = ['relu', 'leaky_relu', 'elu']\n",
        "    best_activation_accuracy = 0\n",
        "    best_activation = None\n",
        "\n",
        "    for activation in activations:\n",
        "        params['activation'] = activation\n",
        "        accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "        if accuracy > best_activation_accuracy:\n",
        "            best_activation_accuracy = accuracy\n",
        "            best_activation = activation\n",
        "\n",
        "    params['activation'] = best_activation\n",
        "\n",
        "    # Explore optimizers\n",
        "    optimizers = ['SGD', 'ADAM', 'RMSprop']\n",
        "    best_optimizer_accuracy = 0\n",
        "    best_optimizer = None\n",
        "\n",
        "    for optimizer in optimizers:\n",
        "        params['optimizer'] = optimizer\n",
        "        accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "        if accuracy > best_optimizer_accuracy:\n",
        "            best_optimizer_accuracy = accuracy\n",
        "            best_optimizer = optimizer\n",
        "\n",
        "    params['optimizer'] = best_optimizer\n",
        "\n",
        "    # Explore L1 and L2 regularization\n",
        "    regularizations = [(0, 0), (0.001, 0), (0, 0.0001)]\n",
        "    best_regularization_accuracy = 0\n",
        "    best_regularization = None\n",
        "\n",
        "    for l1_reg, l2_reg in regularizations:\n",
        "        params['l1_reg'] = l1_reg\n",
        "        params['l2_reg'] = l2_reg\n",
        "        accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "        if accuracy > best_regularization_accuracy:\n",
        "            best_regularization_accuracy = accuracy\n",
        "            best_regularization = (l1_reg, l2_reg)\n",
        "\n",
        "    params['l1_reg'], params['l2_reg'] = best_regularization\n",
        "\n",
        "    # Explore Dropout\n",
        "    dropout_rates = [0, 0.5]\n",
        "    best_dropout_accuracy = 0\n",
        "    best_dropout_rate = None\n",
        "\n",
        "    for dropout_rate in dropout_rates:\n",
        "        params['dropout_rate'] = dropout_rate\n",
        "        accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "        if accuracy > best_dropout_accuracy:\n",
        "            best_dropout_accuracy = accuracy\n",
        "            best_dropout_rate = dropout_rate\n",
        "\n",
        "    params['dropout_rate'] = best_dropout_rate\n",
        "\n",
        "    # Explore Batch Normalization\n",
        "    batch_norm_options = [True, False]\n",
        "    best_batch_norm_accuracy = 0\n",
        "    best_batch_norm = None\n",
        "\n",
        "    for batch_norm in batch_norm_options:\n",
        "        params['batch_normalization'] = batch_norm\n",
        "    accuracy = cross_validation(model_fn, train_dataset, k_folds=k_folds, params=params, criterion=criterion)\n",
        "    if accuracy > best_batch_norm_accuracy:\n",
        "          best_batch_norm_accuracy = accuracy\n",
        "          best_batch_norm = batch_norm\n",
        "\n",
        "    params['batch_normalization'] = best_batch_norm\n",
        "\n",
        "  # Print best hyperparameters and accuracy\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "    print(\"Best accuracy:\", best_accuracy)\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "# Find the best hyperparameters for the MLP model using the training dataset and criterion\n",
        "mlp_best_params, mlp_best_accuracy = find_best_hyperparameters(create_mlp, train_dataset, criterion)\n",
        "\n",
        "# Find the best hyperparameters for the CNN model using the training dataset and criterion\n",
        "cnn_best_params, cnn_best_accuracy = find_best_hyperparameters(create_cnn, train_dataset, criterion)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy for the MLP model\n",
        "print(\"MLP best hyperparameters:\", mlp_best_params)\n",
        "print(\"MLP best accuracy:\", mlp_best_accuracy)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy for the MLP model\n",
        "print(\"CNN best hyperparameters:\", cnn_best_params)\n",
        "print(\"CNN best accuracy:\", cnn_best_accuracy)\n",
        "\n",
        "# Print total training time and if the cpu or gpu was used\n",
        "end_time = time.time()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "print(f\"Total training time: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "sWwWw133nXzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train MLP and CNN models with best hyperparameters\n",
        "mlp_model = create_mlp(mlp_best_params).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = create_optimizer(mlp_model, mlp_best_params)\n",
        "scheduler = create_scheduler(optimizer, mlp_best_params)\n",
        "mlp_train_losses, mlp_train_accuracies, mlp_valid_losses, mlp_valid_accuracies = train_model(mlp_model, train_loader, valid_loader, optimizer, criterion, scheduler, mlp_best_params['epochs'], mlp_best_params)\n",
        "\n",
        "cnn_model = create_cnn(cnn_best_params).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = create_optimizer(cnn_model, cnn_best_params)\n",
        "scheduler = create_scheduler(optimizer, cnn_best_params)\n",
        "cnn_train_losses, cnn_train_accuracies, cnn_valid_losses, cnn_valid_accuracies = train_model(cnn_model, train_loader, valid_loader, optimizer, criterion, scheduler, cnn_best_params['epochs'], cnn_best_params)\n",
        "\n",
        "# Plot the loss function graph with respect to the iteration/epoch\n",
        "plt.figure()\n",
        "plt.plot(mlp_train_losses, label='MLP Train Loss')\n",
        "plt.plot(mlp_valid_losses, label='MLP Validation Loss')\n",
        "plt.plot(cnn_train_losses, label='CNN Train Loss')\n",
        "plt.plot(cnn_valid_losses, label='CNN Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the accuracy graph with respect to the iteration/epoch\n",
        "plt.figure()\n",
        "plt.plot(mlp_train_accuracies, label='MLP Train Accuracy')\n",
        "plt.plot(mlp_valid_accuracies, label='MLP Validation Accuracy')\n",
        "plt.plot(cnn_train_accuracies, label='CNN Train Accuracy')\n",
        "plt.plot(cnn_valid_accuracies, label='CNN Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aUa8opiF5MSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Print predictions and true labels for the top six samples in the testing dataset\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "            for j in range(6):\n",
        "                print(f\"Sample {i * 64 + j + 1}: True label: {labels[j]}, Predicted label: {predicted[j]}\")\n",
        "            \n",
        "            break  # Only go through the first batch\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy, precision, recall, f1 = calculate_metrics(model, test_loader)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "def calculate_metrics(model, test_loader):\n",
        "  # Initialize lists to store true and predicted labels for all test samples\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad(): # Iterate through the test dataset using the test_loader\n",
        "        for inputs, labels in test_loader:\n",
        "          # Move input data and labels to the same device as the model (GPU or CPU)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Pass the inputs through the model to obtain the predicted outputs\n",
        "            outputs = model(inputs)\n",
        "            # Get the class with the highest predicted probability for each input\n",
        "            _, predicted = torch.max(outputs.data, 1) \n",
        "            # Iterate through the test dataset using the test_loader\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate the accuracy, precision, recall, and F1-score using the true and predicted labels\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall, f1 # Return the calculated metrics\n",
        "\n",
        "# Evaluate MLP model\n",
        "print(\"Evaluating MLP model:\")\n",
        "evaluate_model(mlp_model, test_loader)\n",
        "\n",
        "# Evaluate CNN model\n",
        "print(\"\\nEvaluating CNN model:\")\n",
        "evaluate_model(cnn_model, test_loader)\n"
      ],
      "metadata": {
        "id": "11fxCsb69_LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion_matrix(model, test_loader):\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    # Evaluate the model without updating gradients\n",
        "    with torch.no_grad(): \n",
        "      # Iterate through the test dataset using the test_loader\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # Get the class with the highest predicted probability for each input\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Add the true labels and predicted labels to their respective lists\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "# Return the confusion matrix computed using the true and predicted labels\n",
        "    return confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Get confusion matrices for both models\n",
        "mlp_cm = get_confusion_matrix(mlp_model, test_loader)\n",
        "cnn_cm = get_confusion_matrix(cnn_model, test_loader)\n",
        "\n",
        "# Combine confusion matrices\n",
        "combined_cm = mlp_cm + cnn_cm\n",
        "\n",
        "# Plot the combined confusion matrix\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(combined_cm, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Combined Confusion Matrix for MLP and CNN Models')\n"
      ],
      "metadata": {
        "id": "OD-fH0-ZBw5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}